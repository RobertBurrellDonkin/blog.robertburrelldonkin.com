---
title: Cheatsheet – Apache Kafka – ksqlDB
slug: "/cheatsheets/kafka/ksql"
---

# History
 * KSQL -> ksqlDB
    * (push) queries -> push and pull queries
    * DIY Connectors -> Connector Integration

# Key Concepts
 * event streaming relational data source built on Kafka Streams
 * ```KTables``` and ```KStreams``` modelled as **collections**
    * Tables
        * backed by compacted topics
        * ```null``` tombstones are ignored
    * Streams
        * ```null``` values visible
    * **source collections** directly from topics
    * **derived collections** from other collections
 * **SQL** constructs and syntax widely included
 * **push queries** emit events
     * compiled into Kafka Streams applications suitable for microservices
 * **Materialized views** with **pull queries** static snapshot similar to RDBMS
 * **Connector** sinks and sources
    * ETL pipeline use cases
    * **Kafka Connect**
 * Feels like SQL
    * Specialized SQL dialect, not ANSI with enhancements
        * DDL
            * enhanced integration with schema registries
            * scheme evolution
        * DML
            * simple queries but good integration with streams
    * Interactive exploratory interface
    * Network clients
    * Functions and operators
    * eventually consistent

# Architecture
 * **clients** and **servers**

## Servers
 * Cluster independent of brokers
    * scale out, scale in
 * ```kqsl.service.id```
    * same id, same consumer group
    * consumer group mechanism distributes workloads
 * SQL Engine
    * converts SQL into streams topology then runs
 * REST interface
    * for SQL engine
    * optional depending on **deployment mode**
    * required to run interactively
 * Deployment Modes
    * ```Interactive``` **default**
        * submitted through ```command topic```
    * ```Headless```
        * ```queries.file``` points to queries executed on start
 * ```ksql-start-server ksql-server.properties```

### Kafka Connect
 * Modes
    * **Embedded**
        * ```ksql.connect.worker.config``` path to configuration properties
        * co-located
        * workers sames as ksqlDB servers
        * reasons
            * maintain scale and availability together
            * moderate throughput expected
            * simplify architecture
            * share memory and compute resources
        * need to restart after installing plugins
    * **External**
        * ```ksql.connect.url```
        * do need to co-locate
        * sink/source connectors are deployed to external connect cluster
        * reasons
            * high throughput expected
            * separate availability and scaling concerns
            * reuse cluster alraedy deployed

### Properties

#### Key
 * ```listeners``` REST API endpoint
 * ```bootstrap.servers``` Kafka cluster

## Clients
 * ```ksqlDB CLI``` interactive ```ksql host:post```
 * ```ksqlDB UI``` (commercial) visualizer, similar to Toad


# Interactive Mode
 * **statement-based** replication
    * **command topic** distributes statement to nodes

# REST API
 * ```POST /query``` <- JSON

# SQL API

## Automation
 * ```RUN SCRIPT file ;```

## Configuration
 * ```SET```
    * ```SET 'auto.offset.reset' = 'earliest';```

## DDL - Data Definition Language
 * ```SHOW```
    * ```SHOW FUNCTIONS;```
    * ```SHOW TOPICS;```
    * ```SHOW QUERIES;```
        * ```SHOW QUERIES EXTENDED```
    * ```SHOW TYPES ;``` shows in detail custom types defined
    * ```SHOW CONNECTORS ;```
        * ```SHOW SINK CONNECTORS ;```
        * ```SHOW SOURCE CONNECTORS ;```
    * ```SHOW STREAMS ;```
        * ```SHOW STREAMS EXTENDED ;``` more details useful for **diagnostics**
    * ```SHOW TABLES ;```
        * ```SHOW TABLES EXTENDED ;``` more details useful for **diagnostics**
 * ```LIST```
    * ```LIST TYPES ;``` lists custom types defined
    * ```LIST CONNECTORS ;```
        * ```LIST SINK CONNECTORS ;```
        * ```LIST SOURCE CONNECTORS ;```
    * ```LIST STREAMS ;```
    * ```LIST TABLES ;```
    * ```LIST QUERIES;```
 * ```CREATE```
    * **collections**
        * cannot create a stream with the same name as a table or vice versa
        * key can be read into a column identified by
            * ```PRIMARY KEY``` for tables
            * ```KEY``` for streams
        * ```CREATE STREAM name (...) WITH (...)```
            * ```KEY``` to identify key column
        * ```CREATE TABLE name (...) WITH (...)```
            * ```PRIMARY KEY``` to identify key column
        * when schema is stored in registry, types are optional
        * ```WITH```
            * **required** **KAFKA_TOPIC** **VALUE_FORMAT**
            * ```with(KAFKA_TOPIC='name'...)```
            * ```with(...VALUE_FORMAT='format'...)```
            * ```with(...PARTITIONS=number...)```
            * ```with(...REPLICAS=number...)```
            * ```with(...TIMESTAMP='column'...)``` defaults to metadata
            * ```with(...TIMESTAMP_FORMAT='format'...)``` not needed when ```BIGINT```
        * ```IF NOT EXISTS``` optional ```CREATE STREAM IF NOT EXISTS name (...) WITH (...)```
        * **derived collections**
            * ```CREATE ... AS SELECT ...```
            * **derived streams** CSAS
                * ```CREATE STREAM ... AS SELECT ...```
            * **derived tables** CTAS
                * ```CREATE TABLE ... AS SELECT ...```
            * **materialized view** using aggregate query
                * ```CREATE TABLE ... AS SELECT ... GROUP BY ... EMIT CHANGES```
                * pseudo columns
                    * ```WINDOWSTART``` queriable
                    * ```WINDOWEND``` **not** queriable
                * **pull query** limited ```SELECT``` syntax
    * ```CREATE TYPE name AS type``` defines reusable custom type
 * ```DESCRIBE <name> ;``` table or stream
    * ```DESCRIBE EXTENDED <name> ;``` more details
    * ```DESCRIBE FUNCTION <name> ; ```
 * ```DROP```
    * ```DROP TYPE name``` drops a custom type
    * ```DROP CONNECTOR `name` ;```
    * ```DROP STREAM `name` ;```
        * ```DROP STREAM IF EXISTS `name` ;```
        * ```DROP STREAM IF EXISTS `name` DELETE TOPIC;```
    * ```DROP TABLE `name` ;```
        * ```DROP TABLE IF EXISTS `name` ;```
 * ```EXPLAIN```
    * ```EXPLAIN query_id```
    * ```EXPLAIN statement```
 * ```TERMINATE```
    * ```TERMINATE query_id```
    * ```TERMIANTE ALL```

### Connector Integration
 * options for external or internal Kafka Connect cluster
 * ```CREATE SOURCE CONNECTOR `name` with (props);```
 * ```CREATE SINK CONNECTOR `name` with (props);```
 * ```LIST CONNECTORS ;```
    * ```LIST SINK CONNECTORS ;```
    * ```LIST SOURCE CONNECTORS ;```
 * ```SHOW CONNECTORS ;```
    * ```SHOW SINK CONNECTORS ;```
    * ```SHOW SOURCE CONNECTORS ;```
 * ```DESCRIBE CONNECTOR `name` ;``` **fault diagnosis**
 * ```DROP CONNECTOR `name` ;```

## DML - Data Manipulation Language
 * ```INSERT INTO name (names) VALUES (values) ```
    * pseudo-columns
        * ```ROWKEY```
        * ```ROWTIME```
 * ```SELECT```
    * ```SELECT expression FROM name EMIT CHANGES;``` **push query**
        * ```SELECT expression FROM name EMIT CHANGES LIMIT count;``` **push query** with **limit**
    * ```SELECT expression FROM name LEFT JOIN collection ON criteria EMIT CHANGES;``` **push query**
    * ```SELECT expression FROM name WINDOW window EMIT CHANGES;``` **push query**
        * ```WINDOW TUMBLING (SIZE number SECONDS)```
        * ```WINDOW HOPPING (SIZE number SECONDS, ADVANCE BY number SECONDS)```
        * ```WINDOW SESSION (number SECONDS)```
        * ```WINDOW ...(..., GRACE PERIOD number timeunit)``` allows delay before dropping late events
            * **default** window retention period
        * ```WINDOW ...(..., RETENTION number timeunit)``` sets **window retention**
            * less retention, smaller store, reducing impact of restarts and rebalancing
            * must be greater than window size plus grace
            * **stream time** not wallclock
    * ```SELECT expression FROM name GROUP BY grouping EMIT CHANGES;``` **push query**
        * **aggregation**
            * ```SELECT COUNT(*) ... GROUP BY ...``` **count**
                * ```AVG```
                * ```MAX```
                * ```MIN```
                * ```SUM```
                * ```COUNT_DISTINCT```
            * ```SELECT LATEST_BY_OFFSET(...) ... GROUP BY ...``` oldest
            * ```SELECT EARLIEST_BY_OFFSET(...) ... GROUP BY ...``` newest
            * **only** columns in ```GROUP BY``` can be included in projection
            * underlying topic keyed by columns grouped by
    * ```SELECT expression FROM name HAVING having EMIT CHANGES;``` **push query**
    * ```SELECT expression FROM name WHERE where LIKE 'likable%' EMIT CHANGES;``` **push query**
        * ```%``` zero or more characters
    * ```WHERE```
        * ```NOT```
        * ```AND```
        * ```OR```
        * ```BETWEEN```
    * **flatten** ```->``` structs
    * ```SELECT COALESCE(...)```  first not null in list
    * ```SELECT IFNULL(...)```  set value when null
    * ```SELECT CASE WHEN condition THEN result ELSE fallback```
    * **persistence queries** write results to topic
        * use **derived collection**
            * ```CREATE ... AS SELECT ...```
 * ```PRINT```
    * ```PRINT `name` FROM BEGINNING ;```


## Joins
 * types
    * ```INNER JOIN``` share key
    * ```FULL JOIN``` null when no matching key
    * ```LEFT JOIN``` null when no matching right key for left
 * subjects
    * stream to stream
        * **must** be windowed
    * stream to table
        * ```FULL JOIN``` **not supported**
    * table to table
    * tables
        * **must** be joined on ```PRIMARY KEY```
 * constraints
    * same data type
        * ```SELECT ... INNER JOIN ... ON CAST(... AS ...)=``` use an expression to convert to same data type
    * same partition count and strategy
        * workaround by creating new table or stream with compatible partitioning
 * **persistent joins**
    * ```CREATE STREAM ... AS SELECT ... INNER JOIN ... EMIT CHANGES ;"
    * ```CREATE TABLE ... AS SELECT ... LEFT JOIN ...  ;"
 * windowing
    * ```TIMESTAMP``` defined on ```CREATE```
    * ```WITHIN``` **sliding window** based on difference in timestamps
        * ```SELECT ... INNER JOIN ... WITHIN 2 MINUTES ... ;```

## Key Data Types
 * ```ARRAY```
 * ```BOOLEAN```
 * ```INT```
 * ```BIGINT```
 * ```DOUBLE```
 * ```DECIMAL(scale,precision)```
 * ```MAP```
 * ```VARCHAR``` aka ```STRING```
 * ```STRUCT``` structured collection of fields
 * Custom Types
    * ```CREATE TYPE name AS type```

## Functions
 * Custom functions
    * ```kqsl.extension.dir```

# Testing

 * ```ksql-test-runner``` input -> **SQL** -> output end-to-end
    * use container

# Deployment
 * Options

# Monitoring
 * **late** events skipped during windowing are written to server logs
 * **JMX** instrumentation
    * Kafka cluster eg ```kafka_exporter``` -> prometheus
        * under replicated partitions
        * consumer lag
        * offset advancement
        * topic throughput
        * in sync replicas
        * poll time
        * process latency
        * production and consumption rates
    * ```JConsole``` for development
    * Streams and ksql
        * ```-javaagent:``` jmx agent
 * **Logs**
    * Total log rate
    * Error log rate


# Operations
## Reset
 * **Resetting** a ksql application is risky
 * Use ```kafka-consumer-groups``` to identify the correct consumer group
 * ```kafka-streams-application-reset``` **reset tool**
  * **does not** reset application state
  * update consumer offsets
  * skips to end of intermediate topics
  * Deletes **changelog** and **repatition** topics

## Rate Limiting
 * Using **record cache**
    * rate of production limited by per stream thread cache size
    * **record cache** space allows de-duplication of values in nearby offsets